{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "import seaborn as sns\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sahil\\\\Projects\\\\Cars'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_dir = 'C:\\\\Users\\\\sahil\\\\Projects\\\\Cars\\\\fruits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.DataFrame(columns=('name','path','label'))\n",
    "\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\sahil\\\\Projects\\\\Cars\\\\fruits')\n",
    "all_subdirs = [name for name in os.listdir(\".\") if os.path.isdir(name)]\n",
    "\n",
    "\n",
    "\n",
    "#for path, subdirs, files in os.walk('C:\\\\Users\\\\sahil\\\\Projects\\\\Cars\\\\car_data\\\\train'):   \n",
    "for subs in all_subdirs:\n",
    "    for path, subdirs, files in os.walk(subs):         \n",
    "        for name in files:            \n",
    "            training_data = training_data.append(pd.DataFrame({'name': name,'path': path, 'label': subs}, index=[0]), ignore_index=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple_01.jpg</td>\n",
       "      <td>apple</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple_02.jpg</td>\n",
       "      <td>apple</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple_03.jpg</td>\n",
       "      <td>apple</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apple_04.jpg</td>\n",
       "      <td>apple</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple_05.jpg</td>\n",
       "      <td>apple</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6685</th>\n",
       "      <td>strawberry_974.jpg</td>\n",
       "      <td>strawberry</td>\n",
       "      <td>strawberry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6686</th>\n",
       "      <td>strawberry_975.jpg</td>\n",
       "      <td>strawberry</td>\n",
       "      <td>strawberry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6687</th>\n",
       "      <td>strawberry_976.jpg</td>\n",
       "      <td>strawberry</td>\n",
       "      <td>strawberry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6688</th>\n",
       "      <td>strawberry_98.jpg</td>\n",
       "      <td>strawberry</td>\n",
       "      <td>strawberry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6689</th>\n",
       "      <td>strawberry_99.jpg</td>\n",
       "      <td>strawberry</td>\n",
       "      <td>strawberry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6690 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name        path       label\n",
       "0           apple_01.jpg       apple       apple\n",
       "1           apple_02.jpg       apple       apple\n",
       "2           apple_03.jpg       apple       apple\n",
       "3           apple_04.jpg       apple       apple\n",
       "4           apple_05.jpg       apple       apple\n",
       "...                  ...         ...         ...\n",
       "6685  strawberry_974.jpg  strawberry  strawberry\n",
       "6686  strawberry_975.jpg  strawberry  strawberry\n",
       "6687  strawberry_976.jpg  strawberry  strawberry\n",
       "6688   strawberry_98.jpg  strawberry  strawberry\n",
       "6689   strawberry_99.jpg  strawberry  strawberry\n",
       "\n",
       "[6690 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5353 images belonging to 7 classes.\n",
      "Found 1335 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR='C:\\\\Users\\\\sahil\\\\Projects\\\\Cars\\\\fruits'\n",
    "training_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    validation_split= .2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255,validation_split= .2,)\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    target_size=(64,64),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    batch_size=32,\n",
    "    #shuffle=TRUE,\n",
    "    #seed=24\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "validation_generator = training_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    " \n",
    "    target_size=(64,64),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    subset='validation',\n",
    "    #seed=24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1605888   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 1,617,831\n",
      "Trainable params: 1,617,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([    \n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    # The third convolution\n",
    "    #tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    #tf.keras.layers.MaxPooling2D(2,2),\n",
    "    #tf.keras.layers.Dropout(0.2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "  \n",
    "    #  neuron hidden layer\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 168 steps, validate for 42 steps\n",
      "Epoch 1/20\n",
      "168/168 [==============================] - 245s 1s/step - loss: 1.6559 - accuracy: 0.3574 - val_loss: 1.4841 - val_accuracy: 0.4607\n",
      "Epoch 2/20\n",
      "168/168 [==============================] - 35s 207ms/step - loss: 1.3865 - accuracy: 0.4730 - val_loss: 1.3340 - val_accuracy: 0.4959\n",
      "Epoch 3/20\n",
      "168/168 [==============================] - 39s 232ms/step - loss: 1.3138 - accuracy: 0.5078 - val_loss: 1.3803 - val_accuracy: 0.5011\n",
      "Epoch 4/20\n",
      "168/168 [==============================] - 38s 226ms/step - loss: 1.2455 - accuracy: 0.5358 - val_loss: 1.3221 - val_accuracy: 0.4914\n",
      "Epoch 5/20\n",
      "168/168 [==============================] - 38s 226ms/step - loss: 1.2129 - accuracy: 0.5556 - val_loss: 1.2563 - val_accuracy: 0.5236\n",
      "Epoch 6/20\n",
      "168/168 [==============================] - 38s 227ms/step - loss: 1.1642 - accuracy: 0.5739 - val_loss: 1.1941 - val_accuracy: 0.5356\n",
      "Epoch 7/20\n",
      "168/168 [==============================] - 39s 233ms/step - loss: 1.1311 - accuracy: 0.5799 - val_loss: 1.1842 - val_accuracy: 0.5341\n",
      "Epoch 8/20\n",
      "168/168 [==============================] - 37s 222ms/step - loss: 1.1058 - accuracy: 0.5875 - val_loss: 1.1877 - val_accuracy: 0.5573\n",
      "Epoch 9/20\n",
      "168/168 [==============================] - 37s 220ms/step - loss: 1.0795 - accuracy: 0.6034 - val_loss: 1.2222 - val_accuracy: 0.5191\n",
      "Epoch 10/20\n",
      "168/168 [==============================] - 38s 225ms/step - loss: 1.0440 - accuracy: 0.6174 - val_loss: 1.1511 - val_accuracy: 0.5678\n",
      "Epoch 11/20\n",
      "168/168 [==============================] - 37s 223ms/step - loss: 1.0237 - accuracy: 0.6318 - val_loss: 1.2282 - val_accuracy: 0.5491\n",
      "Epoch 12/20\n",
      "168/168 [==============================] - 37s 222ms/step - loss: 1.0058 - accuracy: 0.6352 - val_loss: 1.1711 - val_accuracy: 0.5663\n",
      "Epoch 13/20\n",
      "168/168 [==============================] - 37s 222ms/step - loss: 0.9918 - accuracy: 0.6441 - val_loss: 1.1870 - val_accuracy: 0.5476\n",
      "Epoch 14/20\n",
      "168/168 [==============================] - 38s 225ms/step - loss: 0.9713 - accuracy: 0.6535 - val_loss: 1.2435 - val_accuracy: 0.5386\n",
      "Epoch 15/20\n",
      "168/168 [==============================] - 39s 233ms/step - loss: 0.9398 - accuracy: 0.6613 - val_loss: 1.1654 - val_accuracy: 0.5678\n",
      "Epoch 16/20\n",
      "168/168 [==============================] - 53s 313ms/step - loss: 0.9288 - accuracy: 0.6705 - val_loss: 1.2326 - val_accuracy: 0.5393\n",
      "Epoch 17/20\n",
      "168/168 [==============================] - 227s 1s/step - loss: 0.9164 - accuracy: 0.6742 - val_loss: 1.1686 - val_accuracy: 0.5753\n",
      "Epoch 18/20\n",
      "168/168 [==============================] - 131s 778ms/step - loss: 0.8919 - accuracy: 0.6815 - val_loss: 1.1302 - val_accuracy: 0.5858\n",
      "Epoch 19/20\n",
      "168/168 [==============================] - 75s 446ms/step - loss: 0.8646 - accuracy: 0.6860 - val_loss: 1.1755 - val_accuracy: 0.5581\n",
      "Epoch 20/20\n",
      "168/168 [==============================] - 84s 501ms/step - loss: 0.8451 - accuracy: 0.7002 - val_loss: 1.2275 - val_accuracy: 0.5685\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
    "history = model.fit(train_generator, \n",
    "                    epochs=20, \n",
    "                    #steps_per_epoch=len(train_generator), \n",
    "                    validation_data = validation_generator, \n",
    "                    #verbose = 1, \n",
    "                    #validation_steps=len(validation_generator)\n",
    "                   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
